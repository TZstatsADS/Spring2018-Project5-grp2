control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
model <- train(diagnosis~., data=df.train, method="chaid", preProcess="scale", trControl=control)
install.packages(CHAID)
install.packages("CHAID")
control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
model <- train(diagnosis~., data=df.train, method="rf", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance)
library(corrplot)
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
# adjust the color, size and rotation degree of the text label
tl.col = "black", tl.cex = 0.6, tl.srt = 45,
# adjust the color, format, size of the corrlation display
addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
addrect = 14)
library(corrplot)
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
# adjust the color, size and rotation degree of the text label
tl.col = "black", tl.cex = 0.6, tl.srt = 45,
# adjust the color, format, size of the corrlation display
addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
addrect = 16)
library(corrplot)
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
# adjust the color, size and rotation degree of the text label
tl.col = "black", tl.cex = 0.6, tl.srt = 45,
# adjust the color, format, size of the corrlation display
addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
addrect = 15)
library(corrplot)
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
# adjust the color, size and rotation degree of the text label
tl.col = "black", tl.cex = 0.6, tl.srt = 45,
# adjust the color, format, size of the corrlation display
addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
addrect = 14)
df.train[,"texture_worst"]
df.train[,"texture_worst"]
df.train <- df.train[,feature_selected]
feature_selected <- c("texture_worst", "area_se", "perimeter_worst",
"concave.points_worst", "concavity_worst", "fractal_dimension_se",
"concavity_se", "texture_se", "smoothness_se",
"smoothness_worst", "fractal_dimension_worst", "symmetry_se",
"symmetry_mean", "symmetry_worst")
df.train <- df.train[,feature_selected]
library(caret)
library("klaR")
library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train[,2:31],df.train[,1],size=c(1:30),rfeControl=control)
# Delete first column from dataset because it has no use,and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]
# Factorize the diagnosis attribute
df$diagnosis <- factor(ifelse(df$diagnosis=="B","Benign","Malignant"))
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
# Delete first column from dataset because it has no use,and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]
# Factorize the diagnosis attribute
df$diagnosis <- factor(ifelse(df$diagnosis=="B","Benign","Malignant"))
df$diagnosis <- as.integer(df$diagnosis)-1   #M=1 B=0
# Split entire data into 80% train set and 20% test set
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]
# Check proportion of diagnosis (Benign/Malignant) in train/test sets
#prop.table(table(df.train$diagnosis))
#prop.table(table(df.test$diagnosis))
df.train <- df.train[,c("diagnosis",feature_selected)]
df.train[,-1]
library(caret)
library("klaR")
library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train[,-1],df.train[,1],size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
model <- train(factor(diagnosis)~., data=df.train, method="rf", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance)
library(caret)
library("klaR")
library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train[,-1],factor(df.train[,1]),size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
feature_selected <- c("perimeter_worst", "concave.points_worst", "area_se",
"concavity_worst", "texture_worst", "smoothness_worst",
"symmetry_worst", "concavity_se")
df.train <- df.train[,c("diagnosis",feature_selected)]
library(caret)
library("klaR")
library(randomForest)
control <- rfeControl(functions=nbFuncs, method="cv", number=5)
results <- rfe(df.train[,-1],factor(df.train[,1]),size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
library(caret)
library("klaR")
library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train[,-1],factor(df.train[,1]),size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
ggpairs(df.train, aes(color=diagnosis, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
ggpairs(df.train[,50], aes(color=factor(diagnosis), alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
ggpairs(df.train[1:50,], aes(color=factor(diagnosis), alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
ggpairs(df.train[1:50,], aes(color=c(1,2), alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
df.train$diagnosis <- factor(df.train$diagnosis)
ggpairs(df.train[1:50,], aes(color=diagnosis, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
# Packages that will be used
packages.used <- c("corrplot", "caret", "randomForest")
# Check packages that need to be installed
packages.needed <- setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# Install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE,
repos='http://cran.us.r-project.org')
}
# Load libraries
library("corrplot")
library("caret")
library("randomForest")
# Set working directory to the doc folder
setwd("~/GitHub/Spring2018-Project5-grp_2/doc")
# Load data
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
# Print the head of data
head(df)
# Delete the first column from dataset as id won't be used, and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]
# Factorize the diagnosis attribute
df$diagnosis <- factor(df$diagnosis)
#df$diagnosis <- as.integer(df$diagnosis)-1   # M=1 B=0
# Split entire data into 80% train set and 20% test set
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]
# Check proportion of diagnosis (Benign/Malignant) in train/test sets
#prop.table(table(df.train$diagnosis))
#prop.table(table(df.test$diagnosis))
#library(corrplot)
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
# adjust the color, size and rotation degree of the text label
tl.col = "black", tl.cex = 0.6, tl.srt = 45,
# adjust the color, format, size of the corrlation display
addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
addrect = 14)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
model <- train(factor(diagnosis)~., data=df.train, method="rf", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance)
feature_selected <- c("texture_worst", "area_se", "perimeter_worst",
"concave.points_worst", "concavity_worst", "fractal_dimension_se",
"concavity_se", "texture_se", "smoothness_se",
"smoothness_worst", "fractal_dimension_worst", "symmetry_se",
"symmetry_mean", "symmetry_worst")
df.train <- df.train[,c("diagnosis",feature_selected)]
#library(caret)
#library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train[,-1],factor(df.train[,1]),size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
feature_selected <- c("perimeter_worst", "concave.points_worst", "area_se",
"concavity_worst", "texture_worst", "smoothness_worst",
"symmetry_worst", "concavity_se", "fractal_dimension_worst",
"symmetry_mean", "fractal_dimension_se")
df.train <- df.train[,c("diagnosis",feature_selected)]
df.train
str(df)
summary(df)
library(Hmisc)
describe(df)
# Packages that will be used
packages.used <- c("corrplot", "caret", "randomForest")
# Check packages that need to be installed
packages.needed <- setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# Install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE,
repos='http://cran.us.r-project.org')
}
# Load libraries
library("corrplot")
library("caret")
library("randomForest")
# Set working directory to the doc folder
setwd("~/GitHub/Spring2018-Project5-grp_2/doc")
# Load data
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
# Print the head of data
head(df)
# Load data
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
# Print the head of data
head(df)
summary(df)
# Delete the first column from dataset as id won't be used, and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]
# Factorize the diagnosis attribute
df$diagnosis <- factor(df$diagnosis)
#df$diagnosis <- as.integer(df$diagnosis)-1   # M=1 B=0
# Split entire data into 80% train set and 20% test set
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]
# Check proportion of diagnosis (Benign/Malignant) in train/test sets
#prop.table(table(df.train$diagnosis))
#prop.table(table(df.test$diagnosis))
#library(corrplot)
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
# adjust the color, size and rotation degree of the text label
tl.col = "black", tl.cex = 0.6, tl.srt = 45,
# adjust the color, format, size of the corrlation display
addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
addrect = 14)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
model <- train(factor(diagnosis)~., data=df.train, method="rf", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance)
feature_selected <- c("texture_worst", "area_se", "perimeter_worst",
"concave.points_worst", "concavity_worst", "fractal_dimension_se",
"concavity_se", "texture_se", "smoothness_se",
"smoothness_worst", "fractal_dimension_worst", "symmetry_se",
"symmetry_mean", "symmetry_worst")
df.train <- df.train[,c("diagnosis",feature_selected)]
#library(caret)
#library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train[,-1],factor(df.train[,1]),size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
feature_selected <- c("perimeter_worst", "concave.points_worst", "area_se",
"concavity_worst", "texture_worst", "smoothness_worst",
"symmetry_worst", "concavity_se", "fractal_dimension_worst",
"symmetry_mean", "fractal_dimension_se")
df.train <- df.train[,c("diagnosis",feature_selected)]
source("../lib/xgboost.rmd")
getwd()
source("../lib/xgboost")
source("../lib/xgboost.r")
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
library(xgboost)
install.packages("xgboost")
library(xgboost)
library(magrittr)
library(dplyr)
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
# Load data
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
# Delete the first column from dataset as id won't be used, and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]
# Factorize the diagnosis attribute
df$diagnosis <- factor(df$diagnosis)
#df$diagnosis <- as.integer(df$diagnosis)-1   # M=1 B=0
# Split entire data into 80% train set and 20% test set
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]
# Check proportion of diagnosis (Benign/Malignant) in train/test sets
#prop.table(table(df.train$diagnosis))
#prop.table(table(df.test$diagnosis))
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
df$diagnosis <- as.integer(df$diagnosis)-1   # M=1 B=0
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
df$diagnosis
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
early_stopping_round
early_stopping_round
print_every_n
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgboost(df.train,df.test,run.xg)
dat_train<- df.train
dat_test <- df.test
xgtrain <- xgb.DMatrix(as.matrix(dat_train %>% select(-diagnosis)), label = dat_train$diagnosis)
xgtest  <- xgb.DMatrix(as.matrix(dat_test  %>% select(-diagnosis)), label = dat_test$diagnosis)
params <- list("objective"        = "binary:logistic",
"eval_metric"      = "auc",
"eta"              = 0.012,
"subsample"        = 0.8,
"max_depth"        = 8,
"colsample_bytree" = 0.9,
"min_child_weight" = 5
)
model_xgb.cv <- xgb.cv(params = params,
data = xgtrain,
maximize = TRUE,
nfold = 5,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
print_every_n = 100
early_stopping_round = 100
nrounds = 5000
model_xgb.cv <- xgb.cv(params = params,
data = xgtrain,
maximize = TRUE,
nfold = 5,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
d <- model_xgb.cv$evaluation_log
n <- nrow(d)
v <- model_xgb.cv$best_iteration
dat <- data.frame(x=rep(d$iter, 2), val=c(d$train_auc_mean, d$test_auc_mean), set=rep(c("train", "test"), each=n))
ggplot(data = dat, aes(x=x, y=val)) +
geom_line(aes(colour=set)) +
geom_vline(xintercept=v) +
theme_bw() +
labs(title="AUC values for XGBoost with cross-validation", x="Iteration", y="AUC values")
model_xgb <- xgboost(params = params,
data = xgtrain,
maximize = TRUE,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
xgboost
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgb(df.train,df.test,run.xg)
xgboost(params = params,
data = xgtrain,
maximize = TRUE,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
xgboost
xgboost
View(xgboost)
# Load data
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
# Delete the first column from dataset as id won't be used, and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]
# Factorize the diagnosis attribute
df$diagnosis <- factor(df$diagnosis)
df$diagnosis <- as.integer(df$diagnosis)-1   # M=1 B=0
# Split entire data into 80% train set and 20% test set
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]
# Check proportion of diagnosis (Benign/Malignant) in train/test sets
#prop.table(table(df.train$diagnosis))
#prop.table(table(df.test$diagnosis))
run.xg <- TRUE
source("../lib/xgboost.r")
prediction_xg <- xgb(df.train,df.test,run.xg)
prediction_xg
system.time(xgb(df.train,df.test,run.xg))
Sys.time()
Sys.time()
time1 <- Sys.time()
time2 <- Sys.time()
run.xg <- TRUE
source("../lib/xgboost.r")
output_xg <- xgb(df.train,df.test,run.xg)
output_xg
run.ada <- TRUE
source("../lib/adaboost.r")
output_ada <- adaboost(df.train,df.test,run.ada)
install.packages("ada")
run.xg <- TRUE
source("../lib/xgboost.r")
output_xg <- xgb(df.train,df.test,run.xg)
run.ada <- TRUE
source("../lib/adaboost.r")
output_ada <- adaboost(df.train,df.test,run.ada)
output_ada
plot2 <- ggplot(data = dat1, aes(x=x, y=val)) +
geom_line(colour = "red") +
geom_vline(xintercept=v1) +
theme_bw() +
labs(title="AUC values for XGBoost", x="Iteration", y="AUC values")
pdf('xgboost.pdf',width = 10,height = 10)
plot2
dev.off()
d1 <- model_xgb$evaluation_log
n1 <- nrow(d1)
v1 <- model_xgb$best_iteration
dat1 <- data.frame(x=rep(d1$iter), val=d1$train_auc)
model_xgb <- xgboost(params = params,
data = xgtrain,
maximize = TRUE,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
run.xg <- TRUE
source("../lib/xgboost.r")
output_xg <- xgb(df.train,df.test,run.xg)
dat_train=df.train
dat_test =df.test
## create xgb.DMatrix objects for each trainand test set.
xgtrain <- xgb.DMatrix(as.matrix(dat_train %>% select(-diagnosis)), label = dat_train$diagnosis)
xgtest  <- xgb.DMatrix(as.matrix(dat_test  %>% select(-diagnosis)), label = dat_test$diagnosis)
params <- list("objective"        = "binary:logistic",
"eval_metric"      = "auc",
"eta"              = 0.012,
"subsample"        = 0.8,
"max_depth"        = 8,
"colsample_bytree" = 0.9,
"min_child_weight" = 5
)
## Train the model using cross validation with 5 folds.
model_xgb.cv <- xgb.cv(params = params,
data = xgtrain,
maximize = TRUE,
nfold = 5,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
nrounds = 5000
early_stopping_round = 100
print_every_n = 100
params <- list("objective"        = "binary:logistic",
"eval_metric"      = "auc",
"eta"              = 0.012,
"subsample"        = 0.8,
"max_depth"        = 8,
"colsample_bytree" = 0.9,
"min_child_weight" = 5
)
## Train the model using cross validation with 5 folds.
model_xgb.cv <- xgb.cv(params = params,
data = xgtrain,
maximize = TRUE,
nfold = 5,
nrounds = nrounds,
nthread = 1,
early_stopping_round = early_stopping_round,
print_every_n = print_every_n)
d <- model_xgb.cv$evaluation_log
n <- nrow(d)
v <- model_xgb.cv$best_iteration
dat <- data.frame(x=rep(d$iter, 2), val=c(d$train_auc_mean, d$test_auc_mean), set=rep(c("train", "test"), each=n))
plot1 <- ggplot(data = dat, aes(x=x, y=val)) +
geom_line(aes(colour=set)) +
geom_vline(xintercept=v) +
theme_bw() +
labs(title="AUC values for XGBoost with cross-validation", x="Iteration", y="AUC values")
plot1
pdf('xgboost_cv.pdf',width = 10,height = 10)
pdf('xgboost_cv.pdf',width = 10,height = 10)
plot1
dev.off()
ggsave(plot1, plot = last_plot(), device = NULL, path = "../fig/xgboost_cv.pdf",
scale = 1, width = 10, height = 10, units =c("in", "cm", "mm"),
dpi = 300, limitsize = TRUE, ...)
ggsave(plot1, plot = last_plot(), device = NULL, path = "../fig/xgboost_cv.pdf",
scale = 1, width = 10, height = 10, units =c("in", "cm", "mm"),
dpi = 300, limitsize = TRUE)
?ggsave
ggsave("xgboost_cv.pdf", plot = last_plot(), device = "pdf", path = "../fig/xgboost_cv.pdf",
scale = 1, width = 10, height = 10, units =c("in", "cm", "mm"),
dpi = 300, limitsize = TRUE)
ggsave("xgboost_cv.pdf", plot = last_plot(), device = "pdf", path = "../fig",
scale = 1, width = 10, height = 10, units =c("in", "cm", "mm"),
dpi = 300, limitsize = TRUE)
ggsave("xgboost_cv.pdf", plot = last_plot(), device = "pdf",
scale = 1, width = 10, height = 10, units =c("in", "cm", "mm"),
dpi = 300, limitsize = TRUE)
ggsave("xgboost_cv.pdf", plot = last_plot(), device = "pdf",path = "../figs",
scale = 1, width = 10, height = 10, units =c("in", "cm", "mm"),
dpi = 300, limitsize = TRUE)
run.xg <- TRUE
source("../lib/xgboost.r")
output_xg <- xgb(df.train,df.test,run.xg)
output_xg
