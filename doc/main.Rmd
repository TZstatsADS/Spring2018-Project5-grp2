---
title: "ADA Project 5 - Breast Cancer Classification"
author: "Group 2"
output: html_notebook
---

## Table of Contents  
[Introduction](#Intro)  
[Step 0: Load Packages & Specify Directories](#Step0)  
[Step 1: Load and Process Data](#Step1)  
[Step 2: Feature Selection](#Step2)  
[Step 3: Implement Algorithm](#Step3)  
[Step 4: Evaluation](#Step4)  

### Introduction {#Intro}   
This notebook will examine different classification methods to classify whether the breast cancer is malignant or benign based on the features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. 

### Step 0: Load Packages & Specify Directories {#Step0}  
```{r}
# Packages that will be used
packages.used <- c("corrplot", "caret", "randomForest", "e1071", "dplyr", "gbm")
# Check packages that need to be installed
packages.needed <- setdiff(packages.used, 
                           intersect(installed.packages()[,1], 
                                     packages.used))
# Install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}
# Load libraries  
library("corrplot")
library("caret")
library("randomForest")
library("e1071")
library("dplyr")
library("gbm")
# Set working directory to the doc folder 
setwd("~/GitHub/Spring2018-Project5-grp_2/doc")
```

### Step 1: Load and Process Data {#Step1}  
#### Load Data  
Data Source: [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)
```{r}
# Load data
df <- read.csv("../data/data.csv", header = TRUE, stringsAsFactors = FALSE)
```

#### Explore Data {.tabset}
##### Head  
```{r}
# Print the head of data
head(df)
```
##### Structure  
The dataset includes 569 observations of 33 variables described as below.  
(1) ID number  
(2) Diagnosis (M = malignant, B = benign)   
(3)-(32) Ten real-valued features described as follows are computed for each cell nucleus:  
    a) radius (mean of distances from center to points on the perimeter)  
    b) texture (standard deviation of gray-scale values)  
    c) perimeter  
    d) area  
    e) smoothness (local variation in radius lengths)  
    f) compactness (perimeter^2 / area - 1.0)  
    g) concavity (severity of concave portions of the contour)  
    h) concave points (number of concave portions of the contour)   
    i) symmetry   
    j) fractal dimension ("coastline approximation" - 1)   

The mean, standard error and "worst" or largest (mean of the three largest values) of these features   were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.   
(33) All entries are NA's   

```{r}
# Strucutre of the dataset
str(df)
```
##### Summary
```{r}
# Summary of the dataset
summary(df)
```

#### Process Data  
```{r}
# Delete the first column from dataset as id won't be used, and delete last column from dataset as its entries are all NA's
df <- df[,-c(1,33)]

# Factorize the diagnosis attribute
df$diagnosis <- factor(df$diagnosis)
df$diagnosis <- as.integer(df$diagnosis)-1   # M=1 B=0

# Split entire data into 80% train set and 20% test set
set.seed(123)
index <- sample(1:nrow(df),0.8*nrow(df))
df.train <- df[index,]
df.test <- df[-index,]

# Check proportion of diagnosis (Benign/Malignant) in train/test sets
#prop.table(table(df.train$diagnosis))
#prop.table(table(df.test$diagnosis))
```
### Step 2: Feature Selection {#Step2}  
```{r}
#library(corrplot)
# Compute and plot corrlation matrix
corr_mat <- cor(df.train[,2:ncol(df)])
corrplot(corr_mat, method = "square", order = "hclust",
         # adjust the color, size and rotation degree of the text label
         tl.col = "black", tl.cex = 0.6, tl.srt = 45, 
         # adjust the color, format, size of the corrlation display
         addCoef.col = "black", addCoefasPercent = TRUE, number.cex=0.45,
         addrect = 14)
```
30 features are grouped into 14 groups based on their correlation, and one feature from each group is selected based on their importance. 
a) texture_mean, **texture_worst**  
b) **area_se**, radius_se, perimeter_se  
c) area_mean, radius_mean, perimeter_mean, area_worst, radius_worst, **perimeter_worst**  
d) **concave.points_worst**, concavity_mean, concave.points_mean
e) compactness_mean, compactness_worst, **concavity_worst**   
f) compactness_se, **fractal_dimension_se**  
g) **concavity_se**, concave.points_se  
h) **texture_se**  
i) **smoothness_se**  
j) smoothness_mean, **smoothness_worst**  
k) fractal_dimension_mean, **fractal_dimension_worst**  
l) **symmetry_se**  
m) **symmetry_mean**   
n) **symmetry_worst** 
```{r}
# Compute importance of each feature
control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
model <- train(factor(diagnosis)~., data=df.train, method="rf", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance)
```

```{r}
feature_selected <- c("texture_worst", "area_se", "perimeter_worst", 
                      "concave.points_worst", "concavity_worst", "fractal_dimension_se",
                      "concavity_se", "texture_se", "smoothness_se", 
                      "smoothness_worst", "fractal_dimension_worst", "symmetry_se", 
                      "symmetry_mean", "symmetry_worst")
df.train2 <- df.train[,c("diagnosis",feature_selected)]
```

```{r}
#library(caret)
#library(randomForest)
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
results <- rfe(df.train2[,-1],factor(df.train2[,1]),size=c(1:14),rfeControl=control)
predictors(results)
plot(results, type=c("g", "o"))
```
              
```{r}
feature_selected <- c("perimeter_worst", "concave.points_worst", "area_se",
                      "concavity_worst", "texture_worst", "smoothness_worst",
                      "symmetry_worst", "concavity_se", "fractal_dimension_worst", 
                      "symmetry_mean", "fractal_dimension_se")
df.train2 <- df.train2[,c("diagnosis",feature_selected)]
df.test2 <- df.test[,c("diagnosis",feature_selected)]
```

### Step 3: Implement Algorithm  {#Step3}  

#### algorithms {.tabset}  
##### Random Forest  
```{r}
run.rf <- FALSE
source("../lib/rf.R")

if(run.rf){
  output_rf <- RF(df.train,df.test)
  output2_rf <- RF(df.train2,df.test2)
  
  save(output_rf,file = "../output/output_rf.RData")
  save(output2_rf,file = "../output/output2_rf.RData")
}else{
  load("../output/output_rf.RData")
  load("../output/output2_rf.RData")
}
```

##### Logistic Regression
```{r}
run.logi <- FALSE
source("../lib/logi.R")

if(run.logi){
  output_logi <- logi(df.train, df.test)
  output2_logi <- logi(df.train2, df.test2)
  
  save(output_logi, file = "../output/output_logi.RData")
  save(output2_logi, file = "../output/output2_logi.RData")
}else{
  load("../output/output_logi.RData")
  load("../output/output2_logi.RData")
}
```

##### GBM
```{r}
run.gbm <- FALSE
source("../lib/gbmp.r")

if(run.gbm){
  output_gbm <- gbmp(df.train, df.test)
  output2_gbm <- gbmp(df.train2, df.test2)
  
  save(output_gbm, file = "../output/output_gbm.RData")
  save(output2_gbm, file = "../output/output2_gbm.RData")
}else{
  load("../output/output_gbm.RData")
  load("../output/output2_gbm.RData")
}
```
##### XGBoost  
```{r}
run.xg <- FALSE
source("../lib/xgboost.r")

if(run.xg){
  output_xg <- xgb(df.train,df.test)
  output2_xg <- xgb(df.train2,df.test2)
  
  save(output_xg,file = "../output/output_xg.RData")
  save(output2_xg,file = "../output/output2_xg.RData")
}else{
  load("../output/output_xg.RData")
  load("../output/output2_xg.RData")
}
```
##### AdaBoost  
```{r}
run.ada <- FALSE
source("../lib/adaboost.r")

if(run.ada){
  output_ada <- adaboost(df.train,df.test)
  output2_ada <- adaboost(df.train2,df.test2)
  
  save(output_ada,file = "../output/output_ada.RData")
  save(output2_ada,file = "../output/output2_ada.RData")
}else{
  load("../output/output_ada.RData")
  load("../output/output2_ada.RData")
}
```
##### SVM    
```{r}
run.svm <- FALSE
source("../lib/svm.R")

if(run.svm){
  output_svm <- SVM(df.train,df.test)
  output2_svm <- SVM(df.train2,df.test2)
  
  save(output_svm,file = "../output/output_svm.RData")
  save(output2_svm,file = "../output/output2_svm.RData")
}else{
  load("../output/output_svm.RData")
  load("../output/output2_svm.RData")
}
```

### Step 4: Evaluation  {#Step4} 
```{r}
# compute confusion matrix
cm_rf <- confusionMatrix(output_rf$prediction,df.test$diagnosis)
cm_rf2 <- confusionMatrix(output2_rf$prediction,df.test$diagnosis)
cm_logi <- confusionMatrix(output_logi$prediction,df.test$diagnosis)
cm_logi2 <- confusionMatrix(output2_logi$prediction,df.test$diagnosis)
cm_gbm <- confusionMatrix(output_gbm$prediction,df.test$diagnosis)
cm_gbm2 <- confusionMatrix(output2_gbm$prediction,df.test$diagnosis)
cm_xg <- confusionMatrix(output_xg$prediction,df.test$diagnosis)
cm_xg2 <- confusionMatrix(output2_xg$prediction,df.test$diagnosis)
cm_ada <- confusionMatrix(output_ada$prediction,df.test$diagnosis)
cm_ada2 <- confusionMatrix(output2_ada$prediction,df.test$diagnosis)
cm_svm <- confusionMatrix(output_svm$prediction,df.test$diagnosis)
cm_svm2 <- confusionMatrix(output2_svm$prediction,df.test$diagnosis)

# visualize result
par(mfrow=c(4,3), cex.main=0.1)
fourfoldplot(cm_rf$table, conf.level = 0, margin = 1,
             main = paste0("Random Forest (", round(cm_rf$overall[1]*100), "%)"))
fourfoldplot(cm_logi$table, conf.level = 0, margin = 1,
             main = paste0("Logistic Regression (", round(cm_logi$overall[1]*100), "%)"))
fourfoldplot(cm_gbm$table, conf.level = 0, margin = 1,
             main = paste0("GBM (", round(cm_gbm$overall[1]*100), "%)"))
fourfoldplot(cm_xg$table, conf.level = 0, margin = 1,
             main = paste0("XGBoost (", round(cm_xg$overall[1]*100), "%)"))
fourfoldplot(cm_ada$table, conf.level = 0, margin = 1,
             main = paste0("AdaBoost (", round(cm_ada$overall[1]*100), "%)"))
fourfoldplot(cm_svm$table, conf.level = 0, margin = 1,
             main = paste0("SVM (", round(cm_svm$overall[1]*100), "%)"))


fourfoldplot(cm_rf2$table, conf.level = 0, margin = 1,
             main = paste0("Random Forest (", round(cm_rf2$overall[1]*100), "%)"))
fourfoldplot(cm_logi2$table, conf.level = 0, margin = 1,
             main = paste0("Logistic Regression (", round(cm_logi2$overall[1]*100), "%)"))
fourfoldplot(cm_gbm2$table, conf.level = 0, margin = 1,
             main = paste0("GBM (", round(cm_gbm2$overall[1]*100), "%)"))
fourfoldplot(cm_xg2$table, conf.level = 0, margin = 1,
             main = paste0("XGBoost (", round(cm_xg2$overall[1]*100), "%)"))
fourfoldplot(cm_ada2$table, conf.level = 0, margin = 1,
             main = paste0("AdaBoost (", round(cm_ada2$overall[1]*100), "%)"))
fourfoldplot(cm_svm2$table, conf.level = 0, margin = 1,
             main = paste0("SVM (", round(cm_svm2$overall[1]*100), "%)"))
```

```{r}
time <- c(output_rf$time,output_logi$time,output_gbm$time,output_xg$time,output_ada$time,output_svm$time,
          output2_rf$time,output2_logi$time,output2_gbm$time,output2_xg$time,output2_ada$time,output2_svm$time)
time_comparison <- matrix(time, nrow=2, byrow = TRUE,
                          dimnames = list(c("all_features","reduced_features"),
                                          c("RandomForest","Logistic Regression","GBM",
                                            "XGBoost","AdaBoost","SVM")))
round(time_comparison,2)
```

```{r}
accuracy <- c(cm_rf$overall[1],cm_logi$overall[1],cm_gbm$overall[1],
              cm_xg$overall[1],cm_ada$overall[1],cm_svm$overall[1],
              cm_rf2$overall[1],cm_logi2$overall[1],cm_gbm2$overall[1],
              cm_xg2$overall[1],cm_ada2$overall[1],cm_svm2$overall[1])
accuracy_comparison <- matrix(accuracy, nrow = 2, byrow = TRUE)
round(accuracy_comparison,4)
```

